{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech Recognition using SincNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrfanKhalid/BigData-Spark/blob/master/Speech_Recognition_using_SincNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "78cgCmaDnpx8",
        "colab_type": "code",
        "outputId": "ad74e78e-f6c6-4bf2-98eb-bc15458a5b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pysoundfile"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pysoundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/b3/0b871e5fd31b9a8e54b4ee359384e705a1ca1e2870706d2f081dc7cc1693/PySoundFile-0.9.0.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.6/dist-packages (from pysoundfile) (1.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=0.6->pysoundfile) (2.19)\n",
            "Installing collected packages: pysoundfile\n",
            "Successfully installed pysoundfile-0.9.0.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cp2peScIrikf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "5abd4502-5ec6-4508-c9fb-5c5845fac4f1"
      },
      "cell_type": "code",
      "source": [
        "!pip install https://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from https://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (483.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 483.0MB 32kB/s \n",
            "\u001b[31mfastai 1.0.51 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 0.3.0.post4\n",
            "    Uninstalling torch-0.3.0.post4:\n",
            "      Successfully uninstalled torch-0.3.0.post4\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CLPZgsP4rjVc",
        "colab_type": "code",
        "outputId": "f6bb0665-7731-4295-ae11-57d0fbd8bdc6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-31ff361f-4b73-4e48-9bcd-cb18d5e6ab87\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-31ff361f-4b73-4e48-9bcd-cb18d5e6ab87\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Data.rar to Data.rar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SVsp13YrhPZs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ao2-QXk5sZk_",
        "colab_type": "code",
        "outputId": "0c6a0f0d-1a24-47f3-b45a-7e1560b3dca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install patool"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\r\u001b[K    13% |████▎                           | 10kB 16.3MB/s eta 0:00:01\r\u001b[K    26% |████████▌                       | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    39% |████████████▊                   | 30kB 2.6MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▏          | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▍      | 61kB 2.5MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▋  | 71kB 2.9MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 81kB 3.1MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GeTCGb5WtN5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir Input_Folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xI1zfI04nEdt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir PInput_Folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ltp5GS82vgm7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp Data.rar  Input_Folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HOkJ0HGxvSyt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir Output_Folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gMoRyTvx0HIj",
        "colab_type": "code",
        "outputId": "430d87c7-9fc9-40c7-f7fe-5ec483bf5e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "import patoolib\n",
        "patoolib.extract_archive(\"Input_Folder/Data.rar\", outdir=\"PInput_Folder\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "patool: Extracting Input_Folder/Data.rar ...\n",
            "patool: running /usr/bin/unrar x -- /content/Input_Folder/Data.rar\n",
            "patool:     with cwd='PInput_Folder'\n",
            "patool: ... Input_Folder/Data.rar extracted to `PInput_Folder'.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PInput_Folder'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "0K84BGRE0Zwd",
        "colab_type": "code",
        "outputId": "53938982-fc30-455b-d649-1a7b4e11b420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls 'PInput_Folder/Data'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "174  251  422  652  777  84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gp7FFohErTNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8305da66-ac02-4581-fa3c-78afb65ad3b1"
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# TIMIT_preparation \n",
        "# Mirco Ravanelli \n",
        "# Mila - University of Montreal \n",
        "\n",
        "# July 2018\n",
        "\n",
        "# Description: \n",
        "# This code prepares TIMIT for the following speaker identification experiments. \n",
        "# It removes start and end silences according to the information reported in the *.wrd files and normalizes the amplitude of each sentence.\n",
        " \n",
        "# How to run it:\n",
        "# python TIMIT_preparation.py $TIMIT_FOLDER $OUTPUT_FOLDER data_lists/TIMIT_all.scp \n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def ReadList(list_file):\n",
        " f=open(list_file,\"r\")\n",
        " lines=f.readlines()\n",
        " list_sig=[]\n",
        " for x in lines:\n",
        "    list_sig.append(x.rstrip())\n",
        " f.close()\n",
        " return list_sig\n",
        "\n",
        "def copy_folder(in_folder,out_folder):\n",
        " if not(os.path.isdir(out_folder)):\n",
        "  shutil.copytree(in_folder, out_folder, ignore=ig_f)\n",
        "\n",
        "def ig_f(dir, files):\n",
        " return [f for f in files if os.path.isfile(os.path.join(dir, f))]\n",
        "\n",
        "\n",
        "\n",
        "in_folder=sys.argv[0]\n",
        "out_folder=sys.argv[1]\n",
        "list_file=sys.argv[2]\n",
        "\n",
        "# Read List file\n",
        "list_sig=ReadList(list_file)\n",
        "\n",
        "# Replicate input folder structure to output folder\n",
        "copy_folder(in_folder,out_folder)\n",
        "\n",
        "\n",
        "# Speech Data Reverberation Loop\n",
        "for i in range(len(list_sig)): \n",
        " \n",
        " # Open the wav file\n",
        " wav_file=in_folder+'/'+list_sig[i]\n",
        " [signal, fs] = sf.read(wav_file)\n",
        " signal=signal.astype(np.float64)\n",
        "\n",
        " # Signal normalization\n",
        " signal=signal/np.abs(np.max(signal))\n",
        "\n",
        " # Read wrd file\n",
        " wrd_file=wav_file.replace(\".wav\",\".wrd\")\n",
        " wrd_sig=ReadList(wrd_file)\n",
        " beg_sig=int(wrd_sig[0].split(' ')[0])\n",
        " end_sig=int(wrd_sig[-1].split(' ')[1])\n",
        " \n",
        " # Remove silences\n",
        " signal=signal[beg_sig:end_sig]\n",
        "\n",
        " \n",
        " # Save normalized speech\n",
        " file_out=out_folder+'/'+list_sig[i]\n",
        "\n",
        " sf.write(file_out, signal, fs)\n",
        " \n",
        " print(\"Done %s\" % (file_out))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9212aff3ca4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Replicate input folder structure to output folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcopy_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9212aff3ca4b>\u001b[0m in \u001b[0;36mcopy_folder\u001b[0;34m(in_folder, out_folder)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcopy_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m  \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mig_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mig_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mignore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DkeOCVMz2bvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "\n",
        "def flip(x, dim):\n",
        "    xsize = x.size()\n",
        "    dim = x.dim() + dim if dim < 0 else dim\n",
        "    x = x.contiguous()\n",
        "    x = x.view(-1, *xsize[dim:])\n",
        "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n",
        "                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
        "    return x.view(xsize)\n",
        "\n",
        "\n",
        "def sinc(band,t_right):\n",
        "    y_right= torch.sin(2*math.pi*band*t_right)/(2*math.pi*band*t_right)\n",
        "    y_left= flip(y_right,0)\n",
        "\n",
        "    y=torch.cat([y_left,Variable(torch.ones(1)).cuda(),y_right])\n",
        "\n",
        "    return y\n",
        "    \n",
        "\n",
        "class SincConv_fast(nn.Module):\n",
        "    \"\"\"Sinc-based convolution\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_channels : `int`\n",
        "        Number of input channels. Must be 1.\n",
        "    out_channels : `int`\n",
        "        Number of filters.\n",
        "    kernel_size : `int`\n",
        "        Filter length.\n",
        "    sample_rate : `int`, optional\n",
        "        Sample rate. Defaults to 16000.\n",
        "    Usage\n",
        "    -----\n",
        "    See `torch.nn.Conv1d`\n",
        "    Reference\n",
        "    ---------\n",
        "    Mirco Ravanelli, Yoshua Bengio,\n",
        "    \"Speaker Recognition from raw waveform with SincNet\".\n",
        "    https://arxiv.org/abs/1808.00158\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def to_mel(hz):\n",
        "        return 2595 * np.log10(1 + hz / 700)\n",
        "\n",
        "    @staticmethod\n",
        "    def to_hz(mel):\n",
        "        return 700 * (10 ** (mel / 2595) - 1)\n",
        "\n",
        "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
        "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\n",
        "\n",
        "        super(SincConv_fast,self).__init__()\n",
        "\n",
        "        if in_channels != 1:\n",
        "            #msg = (f'SincConv only support one input channel '\n",
        "            #       f'(here, in_channels = {in_channels:d}).')\n",
        "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
        "            raise ValueError(msg)\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
        "        if kernel_size%2==0:\n",
        "            self.kernel_size=self.kernel_size+1\n",
        "            \n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "\n",
        "        if bias:\n",
        "            raise ValueError('SincConv does not support bias.')\n",
        "        if groups > 1:\n",
        "            raise ValueError('SincConv does not support groups.')\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.min_low_hz = min_low_hz\n",
        "        self.min_band_hz = min_band_hz\n",
        "\n",
        "        # initialize filterbanks such that they are equally spaced in Mel scale\n",
        "        low_hz = 30\n",
        "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
        "\n",
        "        mel = np.linspace(self.to_mel(low_hz),\n",
        "                          self.to_mel(high_hz),\n",
        "                          self.out_channels + 1)\n",
        "        hz = self.to_hz(mel)\n",
        "        \n",
        "\n",
        "        # filter lower frequency (out_channels, 1)\n",
        "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
        "\n",
        "        # filter frequency band (out_channels, 1)\n",
        "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
        "\n",
        "        # Hamming window\n",
        "        #self.window_ = torch.hamming_window(self.kernel_size)\n",
        "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n",
        "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
        "\n",
        "\n",
        "        # (kernel_size, 1)\n",
        "        n = (self.kernel_size - 1) / 2.0\n",
        "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "    def forward(self, waveforms):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n",
        "            Batch of waveforms.\n",
        "        Returns\n",
        "        -------\n",
        "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n",
        "            Batch of sinc filters activations.\n",
        "        \"\"\"\n",
        "\n",
        "        self.n_ = self.n_.to(waveforms.device)\n",
        "\n",
        "        self.window_ = self.window_.to(waveforms.device)\n",
        "\n",
        "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
        "        \n",
        "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
        "        band=(high-low)[:,0]\n",
        "        \n",
        "        f_times_t_low = torch.matmul(low, self.n_)\n",
        "        f_times_t_high = torch.matmul(high, self.n_)\n",
        "\n",
        "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n",
        "        band_pass_center = 2*band.view(-1,1)\n",
        "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
        "        \n",
        "        \n",
        "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
        "\n",
        "        \n",
        "        band_pass = band_pass / (2*band[:,None])\n",
        "        \n",
        "\n",
        "        self.filters = (band_pass).view(\n",
        "            self.out_channels, 1, self.kernel_size)\n",
        "\n",
        "        return F.conv1d(waveforms, self.filters, stride=self.stride,\n",
        "                        padding=self.padding, dilation=self.dilation,\n",
        "                         bias=None, groups=1) \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "class sinc_conv(nn.Module):\n",
        "\n",
        "    def __init__(self, N_filt,Filt_dim,fs):\n",
        "        super(sinc_conv,self).__init__()\n",
        "\n",
        "        # Mel Initialization of the filterbanks\n",
        "        low_freq_mel = 80\n",
        "        high_freq_mel = (2595 * np.log10(1 + (fs / 2) / 700))  # Convert Hz to Mel\n",
        "        mel_points = np.linspace(low_freq_mel, high_freq_mel, N_filt)  # Equally spaced in Mel scale\n",
        "        f_cos = (700 * (10**(mel_points / 2595) - 1)) # Convert Mel to Hz\n",
        "        b1=np.roll(f_cos,1)\n",
        "        b2=np.roll(f_cos,-1)\n",
        "        b1[0]=30\n",
        "        b2[-1]=(fs/2)-100\n",
        "                \n",
        "        self.freq_scale=fs*1.0\n",
        "        self.filt_b1 = nn.Parameter(torch.from_numpy(b1/self.freq_scale))\n",
        "        self.filt_band = nn.Parameter(torch.from_numpy((b2-b1)/self.freq_scale))\n",
        "\n",
        "        \n",
        "        self.N_filt=N_filt\n",
        "        self.Filt_dim=Filt_dim\n",
        "        self.fs=fs\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        filters=Variable(torch.zeros((self.N_filt,self.Filt_dim))).cuda()\n",
        "        N=self.Filt_dim\n",
        "        t_right=Variable(torch.linspace(1, (N-1)/2, steps=int((N-1)/2))/self.fs).cuda()\n",
        "        \n",
        "        \n",
        "        min_freq=50.0;\n",
        "        min_band=50.0;\n",
        "        \n",
        "        filt_beg_freq=torch.abs(self.filt_b1)+min_freq/self.freq_scale\n",
        "        filt_end_freq=filt_beg_freq+(torch.abs(self.filt_band)+min_band/self.freq_scale)\n",
        "       \n",
        "        n=torch.linspace(0, N, steps=N)\n",
        "\n",
        "        # Filter window (hamming)\n",
        "        window=0.54-0.46*torch.cos(2*math.pi*n/N);\n",
        "        window=Variable(window.float().cuda())\n",
        "\n",
        "        \n",
        "        for i in range(self.N_filt):\n",
        "                        \n",
        "            low_pass1 = 2*filt_beg_freq[i].float()*sinc(filt_beg_freq[i].float()*self.freq_scale,t_right)\n",
        "            low_pass2 = 2*filt_end_freq[i].float()*sinc(filt_end_freq[i].float()*self.freq_scale,t_right)\n",
        "            band_pass=(low_pass2-low_pass1)\n",
        "\n",
        "            band_pass=band_pass/torch.max(band_pass)\n",
        "\n",
        "            filters[i,:]=band_pass.cuda()*window\n",
        "\n",
        "        out=F.conv1d(x, filters.view(self.N_filt,1,self.Filt_dim))\n",
        "    \n",
        "        return out\n",
        "    \n",
        "\n",
        "def act_fun(act_type):\n",
        "\n",
        " if act_type==\"relu\":\n",
        "    return nn.ReLU()\n",
        "            \n",
        " if act_type==\"tanh\":\n",
        "    return nn.Tanh()\n",
        "            \n",
        " if act_type==\"sigmoid\":\n",
        "    return nn.Sigmoid()\n",
        "           \n",
        " if act_type==\"leaky_relu\":\n",
        "    return nn.LeakyReLU(0.2)\n",
        "            \n",
        " if act_type==\"elu\":\n",
        "    return nn.ELU()\n",
        "                     \n",
        " if act_type==\"softmax\":\n",
        "    return nn.LogSoftmax(dim=1)\n",
        "        \n",
        " if act_type==\"linear\":\n",
        "    return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n",
        "            \n",
        "            \n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm,self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, options):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        self.input_dim=int(options['input_dim'])\n",
        "        self.fc_lay=options['fc_lay']\n",
        "        self.fc_drop=options['fc_drop']\n",
        "        self.fc_use_batchnorm=options['fc_use_batchnorm']\n",
        "        self.fc_use_laynorm=options['fc_use_laynorm']\n",
        "        self.fc_use_laynorm_inp=options['fc_use_laynorm_inp']\n",
        "        self.fc_use_batchnorm_inp=options['fc_use_batchnorm_inp']\n",
        "        self.fc_act=options['fc_act']\n",
        "        \n",
        "       \n",
        "        self.wx  = nn.ModuleList([])\n",
        "        self.bn  = nn.ModuleList([])\n",
        "        self.ln  = nn.ModuleList([])\n",
        "        self.act = nn.ModuleList([])\n",
        "        self.drop = nn.ModuleList([])\n",
        "       \n",
        "\n",
        "       \n",
        "        # input layer normalization\n",
        "        if self.fc_use_laynorm_inp:\n",
        "           self.ln0=LayerNorm(self.input_dim)\n",
        "          \n",
        "        # input batch normalization    \n",
        "        if self.fc_use_batchnorm_inp:\n",
        "           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n",
        "           \n",
        "           \n",
        "        self.N_fc_lay=len(self.fc_lay)\n",
        "             \n",
        "        current_input=self.input_dim\n",
        "        \n",
        "        # Initialization of hidden layers\n",
        "        \n",
        "        for i in range(self.N_fc_lay):\n",
        "            \n",
        "         # dropout\n",
        "         self.drop.append(nn.Dropout(p=self.fc_drop[i]))\n",
        "         \n",
        "         # activation\n",
        "         self.act.append(act_fun(self.fc_act[i]))\n",
        "         \n",
        "         \n",
        "         add_bias=True\n",
        "         \n",
        "         # layer norm initialization\n",
        "         self.ln.append(LayerNorm(self.fc_lay[i]))\n",
        "         self.bn.append(nn.BatchNorm1d(self.fc_lay[i],momentum=0.05))\n",
        "         \n",
        "         if self.fc_use_laynorm[i] or self.fc_use_batchnorm[i]:\n",
        "             add_bias=False\n",
        "         \n",
        "              \n",
        "         # Linear operations\n",
        "         self.wx.append(nn.Linear(current_input, self.fc_lay[i],bias=add_bias))\n",
        "         \n",
        "         # weight initialization\n",
        "         self.wx[i].weight = torch.nn.Parameter(torch.Tensor(self.fc_lay[i],current_input).uniform_(-np.sqrt(0.01/(current_input+self.fc_lay[i])),np.sqrt(0.01/(current_input+self.fc_lay[i]))))\n",
        "         self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.fc_lay[i]))\n",
        "         \n",
        "         current_input=self.fc_lay[i]\n",
        "         \n",
        "         \n",
        "    def forward(self, x):\n",
        "        \n",
        "      # Applying Layer/Batch Norm\n",
        "      if bool(self.fc_use_laynorm_inp):\n",
        "        x=self.ln0((x))\n",
        "        \n",
        "      if bool(self.fc_use_batchnorm_inp):\n",
        "        x=self.bn0((x))\n",
        "        \n",
        "      for i in range(self.N_fc_lay):\n",
        "\n",
        "        if self.fc_act[i]!='linear':\n",
        "            \n",
        "          if self.fc_use_laynorm[i]:\n",
        "           x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))\n",
        "          \n",
        "          if self.fc_use_batchnorm[i]:\n",
        "           x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))\n",
        "          \n",
        "          if self.fc_use_batchnorm[i]==False and self.fc_use_laynorm[i]==False:\n",
        "           x = self.drop[i](self.act[i](self.wx[i](x)))\n",
        "           \n",
        "        else:\n",
        "          if self.fc_use_laynorm[i]:\n",
        "           x = self.drop[i](self.ln[i](self.wx[i](x)))\n",
        "          \n",
        "          if self.fc_use_batchnorm[i]:\n",
        "           x = self.drop[i](self.bn[i](self.wx[i](x)))\n",
        "          \n",
        "          if self.fc_use_batchnorm[i]==False and self.fc_use_laynorm[i]==False:\n",
        "           x = self.drop[i](self.wx[i](x)) \n",
        "          \n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "class SincNet(nn.Module):\n",
        "    \n",
        "    def __init__(self,options):\n",
        "       super(SincNet,self).__init__()\n",
        "    \n",
        "       self.cnn_N_filt=options['cnn_N_filt']\n",
        "       self.cnn_len_filt=options['cnn_len_filt']\n",
        "       self.cnn_max_pool_len=options['cnn_max_pool_len']\n",
        "       \n",
        "       \n",
        "       self.cnn_act=options['cnn_act']\n",
        "       self.cnn_drop=options['cnn_drop']\n",
        "       \n",
        "       self.cnn_use_laynorm=options['cnn_use_laynorm']\n",
        "       self.cnn_use_batchnorm=options['cnn_use_batchnorm']\n",
        "       self.cnn_use_laynorm_inp=options['cnn_use_laynorm_inp']\n",
        "       self.cnn_use_batchnorm_inp=options['cnn_use_batchnorm_inp']\n",
        "       \n",
        "       self.input_dim=int(options['input_dim'])\n",
        "       \n",
        "       self.fs=options['fs']\n",
        "       \n",
        "       self.N_cnn_lay=len(options['cnn_N_filt'])\n",
        "       self.conv  = nn.ModuleList([])\n",
        "       self.bn  = nn.ModuleList([])\n",
        "       self.ln  = nn.ModuleList([])\n",
        "       self.act = nn.ModuleList([])\n",
        "       self.drop = nn.ModuleList([])\n",
        "       \n",
        "             \n",
        "       if self.cnn_use_laynorm_inp:\n",
        "           self.ln0=LayerNorm(self.input_dim)\n",
        "           \n",
        "       if self.cnn_use_batchnorm_inp:\n",
        "           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n",
        "           \n",
        "       current_input=self.input_dim \n",
        "       \n",
        "       for i in range(self.N_cnn_lay):\n",
        "         \n",
        "         N_filt=int(self.cnn_N_filt[i])\n",
        "         len_filt=int(self.cnn_len_filt[i])\n",
        "         \n",
        "         # dropout\n",
        "         self.drop.append(nn.Dropout(p=self.cnn_drop[i]))\n",
        "         \n",
        "         # activation\n",
        "         self.act.append(act_fun(self.cnn_act[i]))\n",
        "                    \n",
        "         # layer norm initialization         \n",
        "         self.ln.append(LayerNorm([N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])]))\n",
        "\n",
        "         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i]),momentum=0.05))\n",
        "            \n",
        "\n",
        "         if i==0:\n",
        "          self.conv.append(SincConv_fast(self.cnn_N_filt[0],self.cnn_len_filt[0],self.fs))\n",
        "              \n",
        "         else:\n",
        "          self.conv.append(nn.Conv1d(self.cnn_N_filt[i-1], self.cnn_N_filt[i], self.cnn_len_filt[i]))\n",
        "          \n",
        "         current_input=int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])\n",
        "\n",
        "         \n",
        "       self.out_dim=current_input*N_filt\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "       batch=x.shape[0]\n",
        "       seq_len=x.shape[1]\n",
        "       \n",
        "       if bool(self.cnn_use_laynorm_inp):\n",
        "        x=self.ln0((x))\n",
        "        \n",
        "       if bool(self.cnn_use_batchnorm_inp):\n",
        "        x=self.bn0((x))\n",
        "        \n",
        "       x=x.view(batch,1,seq_len)\n",
        "\n",
        "       \n",
        "       for i in range(self.N_cnn_lay):\n",
        "           \n",
        "         if self.cnn_use_laynorm[i]:\n",
        "          if i==0:\n",
        "           x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(torch.abs(self.conv[i](x)), self.cnn_max_pool_len[i]))))  \n",
        "          else:\n",
        "           x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))   \n",
        "          \n",
        "         if self.cnn_use_batchnorm[i]:\n",
        "          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n",
        "\n",
        "         if self.cnn_use_batchnorm[i]==False and self.cnn_use_laynorm[i]==False:\n",
        "          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n",
        "\n",
        "       \n",
        "       x = x.view(batch,-1)\n",
        "\n",
        "       return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MUo5qF8O26xI",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "9820cbee-9a86-4b27-a640-9ec3dd908b38"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17482c4a-a6ec-450a-83b0-8401f3cc04fc\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-17482c4a-a6ec-450a-83b0-8401f3cc04fc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dnn_models.py to dnn_models.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fh6j-UPY4X1Q",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "bc70d2d0-ca67-46cd-f6b7-e650d2ad5a78"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c610622-08e8-453b-ad1b-6fb68e252465\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3c610622-08e8-453b-ad1b-6fb68e252465\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_io.py to data_io.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VAB0V6JP2LNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "685ab84c-5e98-4d3f-b26e-705f2322df77"
      },
      "cell_type": "code",
      "source": [
        "# compute_d_vector.py\n",
        "# Mirco Ravanelli \n",
        "# Mila - University of Montreal \n",
        "\n",
        "# Feb 2019\n",
        "\n",
        "# Description: \n",
        "# This code computes d-vectors using a pre-trained model\n",
        " \n",
        "\n",
        "import os\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from dnn_models import MLP\n",
        "from dnn_models import SincNet as CNN \n",
        "from data_io import ReadList,read_conf_inp,str_to_bool\n",
        "import sys\n",
        "\n",
        "# Model to use for computing the d-vectors\n",
        "model_file='/home/mirco/sincnet_models/SincNet_TIMIT/model_raw.pkl' # This is the model to use for computing the d-vectors (it should be pre-trained using the speaker-id DNN)\n",
        "cfg_file='/home/mirco/SincNet/cfg/SincNet_TIMIT.cfg' # Config file of the speaker-id experiment used to generate the model\n",
        "te_lst='data_lists/TIMIT_test.scp' # List of the wav files to process\n",
        "out_dict_file='d_vect_timit.npy' # output dictionary containing the a sentence id as key as the d-vector as value\n",
        "data_folder='/home/mirco/Dataset/TIMIT_norm_nosil'\n",
        "\n",
        "avoid_small_en_fr=True\n",
        "energy_th = 0.1  # Avoid frames with an energy that is 1/10 over the average energy\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = None\n",
        "\n",
        "# Reading cfg file\n",
        "options=read_conf_inp(cfg_file)\n",
        "\n",
        "\n",
        "#[data]\n",
        "pt_file=options.pt_file\n",
        "output_folder=options.output_folder\n",
        "\n",
        "#[windowing]\n",
        "fs=int(options.fs)\n",
        "cw_len=int(options.cw_len)\n",
        "cw_shift=int(options.cw_shift)\n",
        "\n",
        "#[cnn]\n",
        "cnn_N_filt=list(map(int, options.cnn_N_filt.split(',')))\n",
        "cnn_len_filt=list(map(int, options.cnn_len_filt.split(',')))\n",
        "cnn_max_pool_len=list(map(int, options.cnn_max_pool_len.split(',')))\n",
        "cnn_use_laynorm_inp=str_to_bool(options.cnn_use_laynorm_inp)\n",
        "cnn_use_batchnorm_inp=str_to_bool(options.cnn_use_batchnorm_inp)\n",
        "cnn_use_laynorm=list(map(str_to_bool, options.cnn_use_laynorm.split(',')))\n",
        "cnn_use_batchnorm=list(map(str_to_bool, options.cnn_use_batchnorm.split(',')))\n",
        "cnn_act=list(map(str, options.cnn_act.split(',')))\n",
        "cnn_drop=list(map(float, options.cnn_drop.split(',')))\n",
        "\n",
        "\n",
        "#[dnn]\n",
        "fc_lay=list(map(int, options.fc_lay.split(',')))\n",
        "fc_drop=list(map(float, options.fc_drop.split(',')))\n",
        "fc_use_laynorm_inp=str_to_bool(options.fc_use_laynorm_inp)\n",
        "fc_use_batchnorm_inp=str_to_bool(options.fc_use_batchnorm_inp)\n",
        "fc_use_batchnorm=list(map(str_to_bool, options.fc_use_batchnorm.split(',')))\n",
        "fc_use_laynorm=list(map(str_to_bool, options.fc_use_laynorm.split(',')))\n",
        "fc_act=list(map(str, options.fc_act.split(',')))\n",
        "\n",
        "#[class]\n",
        "class_lay=list(map(int, options.class_lay.split(',')))\n",
        "class_drop=list(map(float, options.class_drop.split(',')))\n",
        "class_use_laynorm_inp=str_to_bool(options.class_use_laynorm_inp)\n",
        "class_use_batchnorm_inp=str_to_bool(options.class_use_batchnorm_inp)\n",
        "class_use_batchnorm=list(map(str_to_bool, options.class_use_batchnorm.split(',')))\n",
        "class_use_laynorm=list(map(str_to_bool, options.class_use_laynorm.split(',')))\n",
        "class_act=list(map(str, options.class_act.split(',')))\n",
        "\n",
        "\n",
        "wav_lst_te=ReadList(te_lst)\n",
        "snt_te=len(wav_lst_te)\n",
        "\n",
        "\n",
        "# Folder creation\n",
        "try:\n",
        "    os.stat(output_folder)\n",
        "except:\n",
        "    os.mkdir(output_folder) \n",
        "    \n",
        "    \n",
        "# loss function\n",
        "cost = nn.NLLLoss()\n",
        "\n",
        "  \n",
        "# Converting context and shift in samples\n",
        "wlen=int(fs*cw_len/1000.00)\n",
        "wshift=int(fs*cw_shift/1000.00)\n",
        "\n",
        "# Batch_dev\n",
        "Batch_dev=128\n",
        "\n",
        "\n",
        "# Feature extractor CNN\n",
        "CNN_arch = {'input_dim': wlen,\n",
        "          'fs': fs,\n",
        "          'cnn_N_filt': cnn_N_filt,\n",
        "          'cnn_len_filt': cnn_len_filt,\n",
        "          'cnn_max_pool_len':cnn_max_pool_len,\n",
        "          'cnn_use_laynorm_inp': cnn_use_laynorm_inp,\n",
        "          'cnn_use_batchnorm_inp': cnn_use_batchnorm_inp,\n",
        "          'cnn_use_laynorm':cnn_use_laynorm,\n",
        "          'cnn_use_batchnorm':cnn_use_batchnorm,\n",
        "          'cnn_act': cnn_act,\n",
        "          'cnn_drop':cnn_drop,          \n",
        "          }\n",
        "\n",
        "CNN_net=CNN(CNN_arch)\n",
        "CNN_net.to(device)\n",
        "\n",
        "\n",
        "\n",
        "DNN1_arch = {'input_dim': CNN_net.out_dim,\n",
        "          'fc_lay': fc_lay,\n",
        "          'fc_drop': fc_drop, \n",
        "          'fc_use_batchnorm': fc_use_batchnorm,\n",
        "          'fc_use_laynorm': fc_use_laynorm,\n",
        "          'fc_use_laynorm_inp': fc_use_laynorm_inp,\n",
        "          'fc_use_batchnorm_inp':fc_use_batchnorm_inp,\n",
        "          'fc_act': fc_act,\n",
        "          }\n",
        "\n",
        "DNN1_net=MLP(DNN1_arch)\n",
        "DNN1_net.to(device)\n",
        "\n",
        "\n",
        "DNN2_arch = {'input_dim':fc_lay[-1] ,\n",
        "          'fc_lay': class_lay,\n",
        "          'fc_drop': class_drop, \n",
        "          'fc_use_batchnorm': class_use_batchnorm,\n",
        "          'fc_use_laynorm': class_use_laynorm,\n",
        "          'fc_use_laynorm_inp': class_use_laynorm_inp,\n",
        "          'fc_use_batchnorm_inp':class_use_batchnorm_inp,\n",
        "          'fc_act': class_act,\n",
        "          }\n",
        "\n",
        "\n",
        "DNN2_net=MLP(DNN2_arch)\n",
        "DNN2_net.to(device)\n",
        "\n",
        "\n",
        "checkpoint_load = torch.load(model_file)\n",
        "CNN_net.load_state_dict(checkpoint_load['CNN_model_par'])\n",
        "DNN1_net.load_state_dict(checkpoint_load['DNN1_model_par'])\n",
        "DNN2_net.load_state_dict(checkpoint_load['DNN2_model_par'])\n",
        "\n",
        "\n",
        "\n",
        "CNN_net.eval()\n",
        "DNN1_net.eval()\n",
        "DNN2_net.eval()\n",
        "test_flag=1 \n",
        "\n",
        "\n",
        "d_vector_dim=fc_lay[-1]\n",
        "d_vect_dict={}\n",
        "\n",
        "   \n",
        "with torch.no_grad(): \n",
        "    \n",
        "    for i in range(snt_te):\n",
        "           \n",
        "         [signal, fs] = sf.read(data_folder+'/'+wav_lst_te[i])\n",
        "         \n",
        "         # Amplitude normalization\n",
        "         signal=signal/np.max(np.abs(signal))\n",
        "        \n",
        "         signal=torch.from_numpy(signal).float().to(device).contiguous()\n",
        "        \n",
        "         if avoid_small_en_fr: \n",
        "             # computing energy on each frame:\n",
        "             beg_samp=0\n",
        "             end_samp=wlen\n",
        "    \n",
        "             N_fr=int((signal.shape[0]-wlen)/(wshift))\n",
        "             Batch_dev=N_fr\n",
        "             en_arr=torch.zeros(N_fr).float().contiguous().to(device)\n",
        "             count_fr=0\n",
        "             count_fr_tot=0\n",
        "             while end_samp<signal.shape[0]:\n",
        "                en_arr[count_fr]=torch.sum(signal[beg_samp:end_samp].pow(2))\n",
        "                beg_samp=beg_samp+wshift\n",
        "                end_samp=beg_samp+wlen\n",
        "                count_fr=count_fr+1\n",
        "                count_fr_tot=count_fr_tot+1\n",
        "                if count_fr==N_fr:\n",
        "                    break\n",
        "    \n",
        "             en_arr_bin=en_arr>torch.mean(en_arr)*0.1\n",
        "             en_arr_bin.to(device)\n",
        "             n_vect_elem=torch.sum(en_arr_bin)\n",
        "    \n",
        "             if n_vect_elem<10:\n",
        "                 print('only few elements used to compute d-vectors')\n",
        "                 sys.exit(0)\n",
        "\n",
        "\n",
        "\n",
        "         # split signals into chunks\n",
        "         beg_samp=0\n",
        "         end_samp=wlen\n",
        "         \n",
        "         N_fr=int((signal.shape[0]-wlen)/(wshift))\n",
        "         \n",
        "        \n",
        "         sig_arr=torch.zeros([Batch_dev,wlen]).float().to(device).contiguous()\n",
        "         dvects=Variable(torch.zeros(N_fr,d_vector_dim).float().to(device).contiguous())\n",
        "         count_fr=0\n",
        "         count_fr_tot=0\n",
        "         while end_samp<signal.shape[0]:\n",
        "             sig_arr[count_fr,:]=signal[beg_samp:end_samp]\n",
        "             beg_samp=beg_samp+wshift\n",
        "             end_samp=beg_samp+wlen\n",
        "             count_fr=count_fr+1\n",
        "             count_fr_tot=count_fr_tot+1\n",
        "             if count_fr==Batch_dev:\n",
        "                 inp=Variable(sig_arr)\n",
        "                 dvects[count_fr_tot-Batch_dev:count_fr_tot,:]=DNN1_net(CNN_net(inp))\n",
        "                 count_fr=0\n",
        "                 sig_arr=torch.zeros([Batch_dev,wlen]).float().to(device).contiguous()\n",
        "           \n",
        "         if count_fr>0:\n",
        "          inp=Variable(sig_arr[0:count_fr])\n",
        "          dvects[count_fr_tot-count_fr:count_fr_tot,:]=DNN1_net(CNN_net(inp))\n",
        "        \n",
        "         if avoid_small_en_fr:\n",
        "             dvects=dvects.index_select(0, (en_arr_bin==1).nonzero().view(-1))\n",
        "         \n",
        "         # averaging and normalizing all the d-vectors\n",
        "         d_vect_out=torch.mean(dvects/dvects.norm(p=2, dim=1).view(-1,1),dim=0)\n",
        "         \n",
        "         # checks for nan\n",
        "         nan_sum=torch.sum(torch.isnan(d_vect_out))\n",
        "\n",
        "         if nan_sum>0:\n",
        "             print(wav_lst_te[i])\n",
        "             sys.exit(0)\n",
        "\n",
        "         \n",
        "         # saving the d-vector in a numpy dictionary\n",
        "         dict_key=wav_lst_te[i].split('/')[-2]+'/'+wav_lst_te[i].split('/')[-1]\n",
        "         d_vect_dict[dict_key]=d_vect_out.cpu().numpy()\n",
        "         print(dict_key)\n",
        "\n",
        "# Save the dictionary\n",
        "np.save(out_dict_file, d_vect_dict)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: ipykernel_launcher.py [options]\n",
            "\n",
            "ipykernel_launcher.py: error: no such option: -f\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Z8W-DEg4WCb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}